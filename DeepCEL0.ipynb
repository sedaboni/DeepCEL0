{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DeepCEL0.ipynb","provenance":[],"collapsed_sections":["tnP7wM79IKW-","jRnQZWSZhArJ","WSV8xnlynp0l","OADNcie-LHxA"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FpCtYevLHfl4"},"source":["# **DeepCEL0 (2D)**\n","\n","---\n","\n","This code is mainly based on Deep-STORM code available at https://github.com/HenriquesLab/ZeroCostDL4Mic\n","\n","<font size = 4>**Deep-STORM** is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by [Nehme *et al.* in Optica](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458). The architecture used here is a U-Net based network without skip connections. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension).\n","\n","<font size = 4>**DeepCEL0** is a deep learning-based algorithm  for  precise  molecule  localization of high density frames acquired by SMLM techniques whose L2 based loss function is regularized by positivity and L0-based constraints. The L0 is relaxed through its Continuous Exact L0 (CEL0) counterpart. The arising approach is parameter-free,  more  flexible,  faster  and  provides  more precise molecule localization maps if compared to Deep-STORM. \n","\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is based on the following paper: \n","\n","<font size = 4>**DeepCEL0 for 2D Single Molecule Localization in Fluorescence Microscopy**, by *Pasquale Cascarano, Maria Colomba Comes, Andrea Sebastiani, Arianna Mencattini, Elena Loli Piccolomini and Eugenio Martinelli*\n","\n","<font size = 4>**Please also cite this paper when using or developing this notebook.**\n","\n","If compared to the original version of the Notebook we have added the following functions used to define the new loss function:\n","\n","1. CEL0L2loss\n","2. compute_norm_ai\n","3. In buildModel we replace the linear activation with ReLu non linearity function. \n"," \n","\n"]},{"cell_type":"markdown","metadata":{"id":"E04mOlG_H5Tz"},"source":["# **1. Initialise the Colab session**\n","---"]},{"cell_type":"code","metadata":{"cellView":"form","id":"gn-LaaNNICqL"},"source":["#@markdown ##Run this cell to check if you have GPU access\n","# %tensorflow_version 1.x\n","\n","import tensorflow as tf\n","# if tf.__version__ != '2.2.0':\n","#   !pip install tensorflow==2.2.0\n","\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime settings are correct then Google did not allocate GPU to your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi\n","\n","# from tensorflow.python.client import device_lib \n","# device_lib.list_local_devices()\n","\n","# print the tensorflow version\n","print('Tensorflow version is ' + str(tf.__version__))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnP7wM79IKW-"},"source":["## **1.2. Mount your Google Drive**\n","---"]},{"cell_type":"code","metadata":{"id":"1R-7Fo34_gOd"},"source":["##Run this cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/DeepCEL0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jRnQZWSZhArJ"},"source":["# **2. Install DeepCEL0 and dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"kSrZMo3X_NhO"},"source":["Notebook_version = ['1.11']\n","\n","##Install DeepCEL0 and dependencies\n","\n","\n","# %% Model definition + helper functions\n","\n","!pip install fpdf\n","# Import keras modules and libraries\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Activation, UpSampling2D, Convolution2D, MaxPooling2D, BatchNormalization, Layer\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import optimizers, losses\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from skimage.transform import warp\n","from skimage.transform import SimilarityTransform\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from scipy.signal import fftconvolve\n","\n","# Import common libraries\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy.io as sio\n","from os.path import abspath\n","from sklearn.model_selection import train_test_split\n","from skimage import io\n","import time\n","import os\n","import shutil\n","import csv\n","from PIL import Image \n","from PIL.TiffTags import TAGS\n","from scipy.ndimage import gaussian_filter\n","import math\n","from astropy.visualization import simple_norm\n","from sys import getsizeof\n","from fpdf import FPDF, HTMLMixin\n","from pip._internal.operations.freeze import freeze\n","import subprocess\n","from datetime import datetime\n","import numpy.matlib\n","\n","\n","# For sliders and dropdown menu, progress bar\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","from tqdm import tqdm\n","\n","# For Multi-threading in simulation\n","from numba import njit, prange\n","\n","\n","def zero_pad(image, shape, position='corner'):\n","    \"\"\"\n","    Extends image to a certain size with zeros\n","    Parameters\n","    ----------\n","    image: real 2d `numpy.ndarray`\n","        input_image image\n","    shape: tuple of int\n","        Desired output shape of the image\n","    position : str, optional\n","        The position of the input_image image in the output one:\n","            * 'corner'\n","                top-left corner (default)\n","            * 'center'\n","                centered\n","    Returns\n","    -------\n","    padded_img: real `numpy.ndarray`\n","        The zero-padded image\n","    \"\"\"\n","    shape = np.asarray(shape, dtype=int)\n","    imshape = np.asarray(image.shape, dtype=int)\n","\n","    if np.alltrue(imshape == shape):\n","        return image\n","\n","    if np.any(shape <= 0):\n","        raise ValueError(\"ZERO_PAD: null or negative shape given\")\n","\n","    dshape = shape - imshape\n","    if np.any(dshape < 0):\n","        raise ValueError(\"ZERO_PAD: target size smaller than source one\")\n","\n","    pad_img = np.zeros(shape, dtype=image.dtype)\n","\n","    idx, idy = np.indices(imshape)\n","\n","    if position == 'center':\n","        if np.any(dshape % 2 != 0):\n","            raise ValueError(\"ZERO_PAD: source and target shapes \"\n","                             \"have different parity.\")\n","        offx, offy = dshape // 2\n","    else:\n","        offx, offy = (0, 0)\n","\n","    pad_img[idx + offx, idy + offy] = image\n","\n","    return pad_img\n","\n","def psf2otf(psf, shape):\n","    \"\"\"\n","    Convert point-spread function to optical transfer function.\n","    Compute the Fast Fourier Transform (FFT) of the point-spread\n","    function (PSF) array and creates the optical transfer function (OTF)\n","    array that is not influenced by the PSF off-centering.\n","    By default, the OTF array is the same size as the PSF array.\n","    To ensure that the OTF is not altered due to PSF off-centering, PSF2OTF\n","    post-pads the PSF array (down or to the right) with zeros to match\n","    dimensions specified in OUTSIZE, then circularly shifts the values of\n","    the PSF array up (or to the left) until the central pixel reaches (1,1)\n","    position.\n","    Parameters\n","    ----------\n","    psf : `numpy.ndarray`\n","        PSF array\n","    shape : int\n","        Output shape of the OTF array\n","    Returns\n","    -------\n","    otf : `numpy.ndarray`\n","        OTF array\n","    Notes\n","    -----\n","    Adapted from MATLAB psf2otf function\n","    \"\"\"\n","    if np.all(psf == 0):\n","        return np.zeros_like(psf)\n","\n","    inshape = psf.shape\n","    # Pad the PSF to outsize\n","    psf = zero_pad(psf, shape, position='corner')\n","\n","    # Circularly shift OTF so that the 'center' of the PSF is\n","    # [0,0] element of the array\n","    for axis, axis_size in enumerate(inshape):\n","        psf = np.roll(psf, -int(axis_size / 2), axis=axis)\n","\n","    # Compute the OTF\n","    otf = np.fft.fft2(psf)\n","\n","    # Estimate the rough number of operations involved in the FFT\n","    # and discard the PSF imaginary part if within roundoff error\n","    # roundoff error  = machine epsilon = sys.float_info.epsilon\n","    # or np.finfo().eps\n","    n_ops = np.sum(psf.size * np.log2(psf.shape))\n","    otf = np.real_if_close(otf, tol=n_ops)\n","\n","    return otf\n","\n","\n","# define a function that projects and rescales an image to the range [0,1]\n","def project_01(im):\n","    im = np.squeeze(im)\n","    min_val = im.min()\n","    max_val = im.max()\n","    return (im - min_val)/(max_val - min_val)\n","\n","# normalize image given mean and std\n","def normalize_im(im, dmean, dstd):\n","    im = np.squeeze(im)\n","    im_norm = np.zeros(im.shape,dtype=np.float32)\n","    im_norm = (im - dmean)/dstd\n","    return im_norm\n","\n","# Define the loss history recorder\n","class LossHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        \n","#  Define a matlab like gaussian 2D filter\n","def matlab_style_gauss2D(shape=(7,7),sigma=1):\n","    \"\"\" \n","    2D gaussian filter - should give the same result as:\n","    MATLAB's fspecial('gaussian',[shape],[sigma]) \n","    \"\"\"\n","    m,n = [(ss-1.)/2. for ss in shape]\n","    y,x = np.ogrid[-m:m+1,-n:n+1]\n","    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n","    h.astype(dtype=K.floatx())\n","    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n","    sumh = h.sum()\n","    if sumh != 0:\n","        h /= sumh\n","    #h = h*2.0\n","    h = h.astype('float32')\n","    return h\n","\n","# Expand the filter dimensions\n","psf_heatmap = matlab_style_gauss2D(shape = (7,7),sigma=1)\n","#psf_heatmap = matlab_style_gauss2D(shape = (7,7),sigma=0.35)\n","gfilter = tf.reshape(psf_heatmap, [7, 7, 1, 1])\n","\n","\n","# Combined MSE + L0 loss\n","def CEL0L2loss(input_shape, norm_ai, weight):\n","    # extraction colonne ==> K.conv2d(e_i, gfilter, strides=(1, 1), padding='same')\n","    # e_i is an element of the base \n","    # to save computational time have to be computed and stored outside the L0 function\n","    # or before the trainin phase!\n","    def bump_mse(heatmap_true, spikes_pred):\n","\n","        # generate the heatmap corresponding to the predicted spikes\n","        heatmap_pred = K.conv2d(spikes_pred, gfilter, strides=(1, 1), padding='same')\n","        #heatmap_pred = spikes_pred\n","        \n","        # heatmaps MSE\n","        loss_heatmaps = losses.mean_squared_error(heatmap_true,heatmap_pred)\n","\n","        # CEL0 on the predicted spikes\n","        norm_ai2 = tf.square(norm_ai)\n","        thresh = np.sqrt(2*weight)/norm_ai\n","        abs_heat = tf.abs(spikes_pred)\n","        bound = tf.square(abs_heat-thresh)\n","        ind = tf.cast(abs_heat<= thresh, tf.float32)\n","        loss_spikes = tf.reduce_mean((weight-0.5*(norm_ai2*bound)*ind))\n","        \n","        return loss_heatmaps + loss_spikes\n","    return bump_mse\n","\n","\n","# Define the concatenated conv2, batch normalization, and relu block\n","def conv_bn_relu(nb_filter, rk, ck, name):\n","    def f(input):\n","        conv = Convolution2D(nb_filter, kernel_size=(rk, ck), strides=(1,1),\\\n","                               padding=\"same\", use_bias=False,\\\n","                               kernel_initializer=\"Orthogonal\",name='conv-'+name)(input)\n","        conv_norm = BatchNormalization(name='BN-'+name)(conv)\n","        conv_norm_relu = Activation(activation = \"relu\",name='Relu-'+name)(conv_norm)\n","        return conv_norm_relu\n","    return f\n","\n","# Define the model architechture\n","def CNN(input,names):\n","    Features1 = conv_bn_relu(32,3,3,names+'F1')(input)\n","    pool1 = MaxPooling2D(pool_size=(2,2),name=names+'Pool1')(Features1)\n","    Features2 = conv_bn_relu(64,3,3,names+'F2')(pool1)\n","    pool2 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool2')(Features2)\n","    Features3 = conv_bn_relu(128,3,3,names+'F3')(pool2)\n","    pool3 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool3')(Features3)\n","    Features4 = conv_bn_relu(512,3,3,names+'F4')(pool3)\n","    up5 = UpSampling2D(size=(2, 2),name=names+'Upsample1')(Features4)\n","    Features5 = conv_bn_relu(128,3,3,names+'F5')(up5)\n","    up6 = UpSampling2D(size=(2, 2),name=names+'Upsample2')(Features5)\n","    Features6 = conv_bn_relu(64,3,3,names+'F6')(up6)\n","    up7 = UpSampling2D(size=(2, 2),name=names+'Upsample3')(Features6)\n","    Features7 = conv_bn_relu(32,3,3,names+'F7')(up7)\n","    return Features7\n","\n","# Define the Model building for an arbitrary input size\n","def buildModel(input_dim, initial_learning_rate = 0.001, L0_weight = 1):\n","    input_ = Input (shape = (input_dim))\n","    act_ = CNN (input_,'CNN')\n","    density_pred = Convolution2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\\\n","                                  activation=\"relu\", use_bias = False,\\\n","                                  kernel_initializer=\"Orthogonal\",name='Prediction')(act_)\n","    model = Model (inputs= input_, outputs=density_pred)\n","    opt = optimizers.Adam(lr = initial_learning_rate)\n","    norm_ai = compute_norm_ai(input_dim, 1, psf_heatmap)\n","    model.compile(optimizer=opt, loss = CEL0L2loss(input_dim, norm_ai, L0_weight))\n","    return model\n","\n","\n","\n","def compute_norm_ai(input_dim, upsampling_factor, psf_heatmap):\n","  norm_ai = np.ones((input_dim[0], input_dim[1]), dtype=np.float32)\n","  otf = psf2otf(psf_heatmap, [input_dim[0],input_dim[1]])\n","  PSF = np.fft.fftshift(np.fft.ifft2(otf))\n","  matr = np.linalg.norm(np.roll(PSF,(0,0)))\n","\n","  return matr*norm_ai\n","\n","\n","# define a function that trains a model for a given data SNR and density\n","def train_model(patches, heatmaps, modelPath, epochs, steps_per_epoch, batch_size, upsampling_factor=8, validation_split = 0.3, initial_learning_rate = 0.001, pretrained_model_path = '', L2_weighting_factor = 100, L0_weight = 1):\n","    \n","    \"\"\"\n","    This function trains a CNN model on the desired training set, given the \n","    upsampled training images and labels generated in MATLAB.\n","    \n","    # Inputs\n","    # TO UPDATE ----------\n","\n","    # Outputs\n","    function saves the weights of the trained model to a hdf5, and the \n","    normalization factors to a mat file. These will be loaded later for testing \n","    the model in test_model.    \n","    \"\"\"\n","    \n","    # for reproducibility\n","    np.random.seed(123)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(patches, heatmaps, test_size = validation_split, random_state=42)\n","    print('Number of training examples: %d' % X_train.shape[0])\n","    print('Number of validation examples: %d' % X_test.shape[0])\n","       \n","    # Setting type\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    y_train = y_train.astype('float32')\n","    y_test = y_test.astype('float32')\n","\n","    \n","    #===================== Training set normalization ==========================\n","    # normalize training images to be in the range [0,1] and calculate the \n","    # training set mean and std\n","    mean_train = np.zeros(X_train.shape[0],dtype=np.float32)\n","    std_train = np.zeros(X_train.shape[0], dtype=np.float32)\n","    for i in range(X_train.shape[0]):\n","        X_train[i, :, :] = project_01(X_train[i, :, :])\n","        mean_train[i] = X_train[i, :, :].mean()\n","        std_train[i] = X_train[i, :, :].std()\n","\n","    # resulting normalized training images\n","    mean_val_train = mean_train.mean()\n","    std_val_train = std_train.mean()\n","    X_train_norm = np.zeros(X_train.shape, dtype=np.float32)\n","    for i in range(X_train.shape[0]):\n","        X_train_norm[i, :, :] = normalize_im(X_train[i, :, :], mean_val_train, std_val_train)\n","    \n","    # patch size\n","    psize = X_train_norm.shape[1]\n","\n","    # Reshaping\n","    X_train_norm = X_train_norm.reshape(X_train.shape[0], psize, psize, 1)\n","\n","    # ===================== Test set normalization ==========================\n","    # normalize test images to be in the range [0,1] and calculate the test set \n","    # mean and std\n","    mean_test = np.zeros(X_test.shape[0],dtype=np.float32)\n","    std_test = np.zeros(X_test.shape[0], dtype=np.float32)\n","    for i in range(X_test.shape[0]):\n","        X_test[i, :, :] = project_01(X_test[i, :, :])\n","        mean_test[i] = X_test[i, :, :].mean()\n","        std_test[i] = X_test[i, :, :].std()\n","\n","    # resulting normalized test images\n","    mean_val_test = mean_test.mean()\n","    std_val_test = std_test.mean()\n","    X_test_norm = np.zeros(X_test.shape, dtype=np.float32)\n","    for i in range(X_test.shape[0]):\n","        X_test_norm[i, :, :] = normalize_im(X_test[i, :, :], mean_val_test, std_val_test)\n","        \n","    # Reshaping\n","    X_test_norm = X_test_norm.reshape(X_test.shape[0], psize, psize, 1)\n","\n","    # Reshaping labels\n","    Y_train = y_train.reshape(y_train.shape[0], psize, psize, 1)\n","    Y_test = y_test.reshape(y_test.shape[0], psize, psize, 1)\n","\n","    # Save datasets to a matfile to open later in matlab\n","    mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test, \"upsampling_factor\": upsampling_factor, \"Normalization factor\": L2_weighting_factor}\n","    sio.savemat(os.path.join(modelPath,\"model_metadata.mat\"), mdict)\n","\n","\n","    # Set the dimensions ordering according to tensorflow consensous\n","    # K.set_image_dim_ordering('tf')\n","    K.set_image_data_format('channels_last')\n","\n","    # Save the model weights after each epoch if the validation loss decreased\n","    checkpointer = ModelCheckpoint(filepath=os.path.join(modelPath,\"weights_best.hdf5\"), verbose=1,\n","                                   save_best_only=True)\n","\n","    # Change learning when loss reaches a plataeu\n","    change_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00005)\n","    \n","    # Model building and complitation\n","    model = buildModel((psize, psize, 1), initial_learning_rate = initial_learning_rate, L0_weight = L0_weight)\n","    model.summary()\n","\n","    # Load pretrained model\n","    if not pretrained_model_path:\n","      print('Using random initial model weights.')\n","    else:\n","      print('Loading model weights from '+pretrained_model_path)\n","      model.load_weights(pretrained_model_path)\n","    \n","    # Create an image data generator for real time data augmentation\n","    datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=0.,  # randomly rotate images in the range (degrees, 0 to 180)\n","        width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n","        zoom_range=0.,\n","        shear_range=0.,\n","        horizontal_flip=False,  # randomly flip images\n","        vertical_flip=False,  # randomly flip images\n","        fill_mode='constant',\n","        data_format=K.image_data_format())\n","\n","    # Fit the image generator on the training data\n","    datagen.fit(X_train_norm)\n","    \n","    # loss history recorder\n","    history = LossHistory()\n","\n","    # Inform user training begun\n","    print('-------------------------------')\n","    print('Training model...')\n","\n","    # Fit model on the batches generated by datagen.flow()\n","    train_history = model.fit_generator(datagen.flow(X_train_norm, Y_train, batch_size=batch_size), \n","                                        steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n","                                        validation_data=(X_test_norm, Y_test), \n","                                        callbacks=[history, checkpointer, change_lr])    \n","\n","    # Inform user training ended\n","    print('-------------------------------')\n","    print('Training Complete!')\n","    \n","    # Save the last model\n","    model.save(os.path.join(modelPath, 'weights_last.hdf5'))\n","\n","    # convert the history.history dict to a pandas DataFrame:     \n","    lossData = pd.DataFrame(train_history.history) \n","\n","    if os.path.exists(os.path.join(modelPath,\"Quality Control\")):\n","      shutil.rmtree(os.path.join(modelPath,\"Quality Control\"))\n","\n","    os.makedirs(os.path.join(modelPath,\"Quality Control\"))\n","\n","    # The training evaluation.csv is saved (overwrites the Files if needed). \n","    lossDataCSVpath = os.path.join(modelPath,\"Quality Control/training_evaluation.csv\")\n","    with open(lossDataCSVpath, 'w') as f:\n","      writer = csv.writer(f)\n","      writer.writerow(['loss','val_loss','learning rate'])\n","      for i in range(len(train_history.history['loss'])):\n","        writer.writerow([train_history.history['loss'][i], train_history.history['val_loss'][i], train_history.history['lr'][i]])\n","\n","    return\n","\n","\n","# Normalization functions from Martin Weigert used in CARE\n","def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    \"\"\"Percentile-based image normalization.\"\"\"\n","\n","    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n","    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","    try:\n","        import numexpr\n","        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","    except ImportError:\n","        x =                   (x - mi) / ( ma - mi + eps )\n","\n","    if clip:\n","        x = np.clip(x,0,1)\n","\n","    return x\n","\n","def norm_minmse(gt, x, normalize_gt=True):\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","\n","    \"\"\"\n","    normalizes and affinely scales an image pair such that the MSE is minimized  \n","     \n","    Parameters\n","    ----------\n","    gt: ndarray\n","        the ground truth image      \n","    x: ndarray\n","        the image that will be affinely scaled \n","    normalize_gt: bool\n","        set to True of gt image should be normalized (default)\n","    Returns\n","    -------\n","    gt_scaled, x_scaled \n","    \"\"\"\n","    if normalize_gt:\n","        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n","    x = x.astype(np.float32, copy=False) - np.mean(x)\n","    #x = x - np.mean(x)\n","    gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n","    #gt = gt - np.mean(gt)\n","    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n","    return gt, scale * x\n","\n","\n","# Multi-threaded Erf-based image construction\n","@njit(parallel=True)\n","def FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = (64,64), pixel_size = 100):\n","  w = image_size[0]\n","  h = image_size[1]\n","  erfImage = np.zeros((w, h))\n","  for ij in prange(w*h):\n","    j = int(ij/w)\n","    i = ij - j*w\n","    for (xc, yc, photon, sigma) in zip(xc_array, yc_array, photon_array, sigma_array):\n","      # Don't bother if the emitter has photons <= 0 or if Sigma <= 0\n","      if (sigma > 0) and (photon > 0):\n","        S = sigma*math.sqrt(2)\n","        x = i*pixel_size - xc\n","        y = j*pixel_size - yc\n","        # Don't bother if the emitter is further than 4 sigma from the centre of the pixel\n","        if (x+pixel_size/2)**2 + (y+pixel_size/2)**2 < 16*sigma**2:\n","          ErfX = math.erf((x+pixel_size)/S) - math.erf(x/S)\n","          ErfY = math.erf((y+pixel_size)/S) - math.erf(y/S)\n","          erfImage[j][i] += 0.25*photon*ErfX*ErfY\n","  return erfImage\n","\n","\n","@njit(parallel=True)\n","def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = (64,64), pixel_size = 100):\n","  w = image_size[0]\n","  h = image_size[1]\n","  locImage = np.zeros((image_size[0],image_size[1]) )\n","  n_locs = len(xc_array)\n","\n","  for e in prange(n_locs):\n","    locImage[int(max(min(round(yc_array[e]/pixel_size),w-1),0))][int(max(min(round(xc_array[e]/pixel_size),h-1),0))] += 1\n","\n","  return locImage\n","\n","\n","\n","def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n","  with Image.open(TIFFpath) as img:\n","    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag.keys()}\n","\n","\n","  # TIFF tags\n","  # https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml\n","  # https://www.awaresystems.be/imaging/tiff/tifftags/resolutionunit.html\n","  ResolutionUnit = meta_dict['ResolutionUnit'][0] # unit of resolution\n","  width = meta_dict['ImageWidth'][0]\n","  height = meta_dict['ImageLength'][0]\n","\n","  xResolution = meta_dict['XResolution'][0] # number of pixels / ResolutionUnit\n","\n","  if len(xResolution) == 1:\n","    xResolution = xResolution[0]\n","  elif len(xResolution) == 2:\n","    xResolution = xResolution[0]/xResolution[1]\n","  else:\n","    print('Image resolution not defined.')\n","    xResolution = 1\n","\n","  if ResolutionUnit == 2:\n","    # Units given are in inches\n","    pixel_size = 0.025*1e9/xResolution\n","  elif ResolutionUnit == 3:\n","    # Units given are in cm\n","    pixel_size = 0.01*1e9/xResolution\n","  else: \n","    # ResolutionUnit is therefore 1\n","    print('Resolution unit not defined. Assuming: um')\n","    pixel_size = 1e3/xResolution\n","\n","  if display:\n","    print('Pixel size obtained from metadata: '+str(pixel_size)+' nm')\n","    print('Image size: '+str(width)+'x'+str(height))\n","  \n","  return (pixel_size, width, height)\n","\n","\n","def saveAsTIF(path, filename, array, pixel_size):\n","  \"\"\"\n","  Image saving using PIL to save as .tif format\n","  # Input \n","  path       - path where it will be saved\n","  filename   - name of the file to save (no extension)\n","  array      - numpy array conatining the data at the required format\n","  pixel_size - physical size of pixels in nanometers (identical for x and y)\n","  \"\"\"\n","\n","  # print('Data type: '+str(array.dtype))\n","  if (array.dtype == np.uint16):\n","    mode = 'I;16'\n","  elif (array.dtype == np.uint32):\n","    mode = 'I'\n","  else:\n","    mode = 'F'\n","\n","  # Rounding the pixel size to the nearest number that divides exactly 1cm.\n","  # Resolution needs to be a rational number --> see TIFF format\n","  # pixel_size = 10000/(round(10000/pixel_size))\n","\n","  if len(array.shape) == 2:\n","    im = Image.fromarray(array)\n","    im.save(os.path.join(path, filename+'.tif'),\n","                  mode = mode,  \n","                  resolution_unit = 3,\n","                  resolution = 0.01*1e9/pixel_size)\n","\n","\n","  elif len(array.shape) == 3:\n","    imlist = []\n","    for frame in array:\n","      imlist.append(Image.fromarray(frame))\n","\n","    imlist[0].save(os.path.join(path, filename+'.tif'), save_all=True,\n","                  append_images=imlist[1:],\n","                  mode = mode,  \n","                  resolution_unit = 3,\n","                  resolution = 0.01*1e9/pixel_size)\n","\n","  return\n","\n","\n","\n","\n","class Maximafinder(Layer):\n","    def __init__(self, thresh, neighborhood_size, use_local_avg, **kwargs):\n","        super(Maximafinder, self).__init__(**kwargs)\n","        self.thresh = tf.constant(thresh, dtype=tf.float32)\n","        self.nhood = neighborhood_size\n","        self.use_local_avg = use_local_avg\n","\n","    def build(self, input_shape):\n","        if self.use_local_avg is True:\n","          self.kernel_x = tf.reshape(tf.constant([[-1,0,1],[-1,0,1],[-1,0,1]], dtype=tf.float32), [3, 3, 1, 1])\n","          self.kernel_y = tf.reshape(tf.constant([[-1,-1,-1],[0,0,0],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n","          self.kernel_sum = tf.reshape(tf.constant([[1,1,1],[1,1,1],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n","\n","    def call(self, inputs):\n","\n","        # local maxima positions\n","        max_pool_image = MaxPooling2D(pool_size=(self.nhood,self.nhood), strides=(1,1), padding='same')(inputs)\n","        cond = tf.math.greater(max_pool_image, self.thresh) & tf.math.equal(max_pool_image, inputs)\n","        indices = tf.where(cond)\n","        bind, xind, yind = indices[:, 0], indices[:, 2], indices[:, 1]\n","        confidence = tf.gather_nd(inputs, indices)\n","\n","        # local CoG estimator\n","        if self.use_local_avg:\n","          x_image = K.conv2d(inputs, self.kernel_x, padding='same')\n","          y_image = K.conv2d(inputs, self.kernel_y, padding='same')\n","          sum_image = K.conv2d(inputs, self.kernel_sum, padding='same')\n","          confidence = tf.cast(tf.gather_nd(sum_image, indices), dtype=tf.float32)\n","          x_local = tf.math.divide(tf.gather_nd(x_image, indices),tf.gather_nd(sum_image, indices))\n","          y_local = tf.math.divide(tf.gather_nd(y_image, indices),tf.gather_nd(sum_image, indices))\n","          xind = tf.cast(xind, dtype=tf.float32) + tf.cast(x_local, dtype=tf.float32)\n","          yind = tf.cast(yind, dtype=tf.float32) + tf.cast(y_local, dtype=tf.float32)\n","        else:\n","          xind = tf.cast(xind, dtype=tf.float32)\n","          yind = tf.cast(yind, dtype=tf.float32)\n","        \n","        return bind, xind, yind, confidence\n","\n","    def get_config(self):\n","\n","        # Implement get_config to enable serialization. This is optional.\n","        base_config = super(Maximafinder, self).get_config()\n","        config = {}\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","\n","# ------------------------------- Prediction with postprocessing  function-------------------------------\n","def batchFramePredictionLocalization(dataPath, filename, modelPath, savePath, batch_size=1, thresh=0.1, neighborhood_size=3, use_local_avg = False, pixel_size = None):\n","    \"\"\"\n","    This function tests a trained model on the desired test set, given the \n","    tiff stack of test images, learned weights, and normalization factors.\n","    \n","    # Inputs\n","    dataPath          - the path to the folder containing the tiff stack(s) to run prediction on \n","    filename          - the name of the file to process\n","    modelPath         - the path to the folder containing the weights file and the mean and standard deviation file generated in train_model\n","    savePath          - the path to the folder where to save the prediction\n","    batch_size.       - the number of frames to predict on for each iteration\n","    thresh            - threshoold percentage from the maximum of the gaussian scaling\n","    neighborhood_size - the size of the neighborhood for local maxima finding\n","    use_local_average - Boolean whether to perform local averaging or not\n","    \"\"\"\n","    \n","    # load mean and std\n","    matfile = sio.loadmat(os.path.join(modelPath,'model_metadata.mat'))\n","    test_mean = np.array(matfile['mean_test'])\n","    test_std = np.array(matfile['std_test'])  \n","    upsampling_factor = np.array(matfile['upsampling_factor'])\n","    upsampling_factor = upsampling_factor.item() # convert to scalar\n","    L2_weighting_factor = np.array(matfile['Normalization factor'])\n","    L2_weighting_factor = L2_weighting_factor.item() # convert to scalar\n","\n","    # Read in the raw file\n","    Images = io.imread(os.path.join(dataPath, filename))\n","    if pixel_size == None:\n","      pixel_size, _, _ = getPixelSizeTIFFmetadata(os.path.join(dataPath, filename), display=True)\n","    pixel_size_hr = pixel_size/upsampling_factor\n","\n","    # get dataset dimensions\n","    (nFrames, M, N) = Images.shape\n","    print('Input image is '+str(N)+'x'+str(M)+' with '+str(nFrames)+' frames.')\n","\n","    # Build the model for a bigger image\n","    model = buildModel((upsampling_factor*M, upsampling_factor*N, 1))\n","\n","    # Load the trained weights\n","    model.load_weights(os.path.join(modelPath,'weights_best.hdf5'))\n","\n","    # add a post-processing module\n","    max_layer = Maximafinder(thresh*L2_weighting_factor, neighborhood_size, use_local_avg)\n","\n","    # Initialise the results: lists will be used to collect all the localizations\n","    frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n","\n","    # Initialise the results\n","    Prediction = np.zeros((M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","    Widefield = np.zeros((M, N), dtype=np.float32)\n","\n","    # run model in batches\n","    n_batches = math.ceil(nFrames/batch_size)\n","    for b in tqdm(range(n_batches)):\n","\n","      nF = min(batch_size, nFrames - b*batch_size)\n","      Images_norm = np.zeros((nF, M, N),dtype=np.float32)\n","      Images_upsampled = np.zeros((nF, M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","\n","      # Upsampling using a simple nearest neighbor interp and calculating - MULTI-THREAD this?\n","      for f in range(nF):\n","        Images_norm[f,:,:] = project_01(Images[b*batch_size+f,:,:])\n","        Images_norm[f,:,:] = normalize_im(Images_norm[f,:,:], test_mean, test_std)\n","        Images_upsampled[f,:,:] = np.kron(Images_norm[f,:,:], np.ones((upsampling_factor,upsampling_factor)))\n","        Widefield += Images[b*batch_size+f,:,:]\n","\n","      # Reshaping\n","      Images_upsampled = np.expand_dims(Images_upsampled,axis=3)\n","\n","      # Run prediction and local amxima finding\n","      predicted_density = model.predict_on_batch(Images_upsampled)\n","      predicted_density[predicted_density < 0] = 0\n","      Prediction += predicted_density.sum(axis = 3).sum(axis = 0)\n","\n","      bind, xind, yind, confidence = max_layer(predicted_density)\n","      \n","      # normalizing the confidence by the L2_weighting_factor\n","      confidence /= L2_weighting_factor \n","\n","      # turn indices to nms and append to the results\n","      xind, yind = xind*pixel_size_hr, yind*pixel_size_hr\n","      frmind = (bind.numpy() + b*batch_size + 1).tolist()\n","      xind = xind.numpy().tolist()\n","      yind = yind.numpy().tolist()\n","      confidence = confidence.numpy().tolist()\n","      frame_number_list += frmind\n","      x_nm_list += xind\n","      y_nm_list += yind\n","      confidence_au_list += confidence\n","\n","    # Open and create the csv file that will contain all the localizations\n","    if use_local_avg:\n","      ext = '_avg'\n","    else:\n","      ext = '_max'\n","    with open(os.path.join(savePath, 'Localizations_' + os.path.splitext(filename)[0] + ext + '.csv'), \"w\", newline='') as file:\n","      writer = csv.writer(file)\n","      writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n","      locs = list(zip(frame_number_list, x_nm_list, y_nm_list, confidence_au_list))\n","      writer.writerows(locs)\n","\n","    # Save the prediction and widefield image\n","    Widefield = np.kron(Widefield, np.ones((upsampling_factor,upsampling_factor)))\n","    Widefield = np.float32(Widefield)\n","\n","    # io.imsave(os.path.join(savePath, 'Predicted_'+os.path.splitext(filename)[0]+'.tif'), Prediction)\n","    # io.imsave(os.path.join(savePath, 'Widefield_'+os.path.splitext(filename)[0]+'.tif'), Widefield)\n","\n","    saveAsTIF(savePath, 'Predicted_'+os.path.splitext(filename)[0], Prediction, pixel_size_hr)\n","    saveAsTIF(savePath, 'Widefield_'+os.path.splitext(filename)[0], Widefield, pixel_size_hr)\n","\n","\n","    return\n","\n","# ------------------------------- Prediction with postprocessing  function and stack-------------------------------\n","def batchFramesPredictionLocalization(dataPath, filename, modelPath, savePath, batch_size=1, thresh=0.1, neighborhood_size=3, use_local_avg = False, pixel_size = None):\n","    \"\"\"\n","    This function tests a trained model on the desired test set, given the \n","    tiff stack of test images, learned weights, and normalization factors.\n","    \n","    # Inputs\n","    dataPath          - the path to the folder containing the tiff stack(s) to run prediction on \n","    filename          - the name of the file to process\n","    modelPath         - the path to the folder containing the weights file and the mean and standard deviation file generated in train_model\n","    savePath          - the path to the folder where to save the prediction\n","    batch_size.       - the number of frames to predict on for each iteration\n","    thresh            - threshoold percentage from the maximum of the gaussian scaling\n","    neighborhood_size - the size of the neighborhood for local maxima finding\n","    use_local_average - Boolean whether to perform local averaging or not\n","    \"\"\"\n","    \n","    # load mean and std\n","    matfile = sio.loadmat(os.path.join(modelPath,'model_metadata.mat'))\n","    test_mean = np.array(matfile['mean_test'])\n","    test_std = np.array(matfile['std_test'])  \n","    upsampling_factor = np.array(matfile['upsampling_factor'])\n","    upsampling_factor = upsampling_factor.item() # convert to scalar\n","    L2_weighting_factor = np.array(matfile['Normalization factor'])\n","    L2_weighting_factor = L2_weighting_factor.item() # convert to scalar\n","\n","    # Read in the raw file\n","    Images = io.imread(os.path.join(dataPath, filename))\n","    if pixel_size == None:\n","      pixel_size, _, _ = getPixelSizeTIFFmetadata(os.path.join(dataPath, filename), display=True)\n","    pixel_size_hr = pixel_size/upsampling_factor\n","\n","    # get dataset dimensions\n","    (nFrames, M, N) = Images.shape\n","    print('Input image is '+str(N)+'x'+str(M)+' with '+str(nFrames)+' frames.')\n","\n","    # Build the model for a bigger image\n","    model = buildModel((upsampling_factor*M, upsampling_factor*N, 1))\n","\n","    # Load the trained weights\n","    model.load_weights(os.path.join(modelPath,'weights_best.hdf5'))\n","\n","    # add a post-processing module\n","    max_layer = Maximafinder(thresh*L2_weighting_factor, neighborhood_size, use_local_avg)\n","\n","    # Initialise the results: lists will be used to collect all the localizations\n","    frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n","\n","    # Initialise the results\n","    Prediction = np.zeros((M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","    Widefield = np.zeros((M, N), dtype=np.float32)\n","\n","    Stack = list()\n","\n","    # run model in batches\n","    n_batches = math.ceil(nFrames/batch_size)\n","    for b in tqdm(range(n_batches)):\n","\n","      nF = min(batch_size, nFrames - b*batch_size)\n","      Images_norm = np.zeros((nF, M, N),dtype=np.float32)\n","      Images_upsampled = np.zeros((nF, M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n","\n","      # Upsampling using a simple nearest neighbor interp and calculating - MULTI-THREAD this?\n","      for f in range(nF):\n","        Images_norm[f,:,:] = project_01(Images[b*batch_size+f,:,:])\n","        Images_norm[f,:,:] = normalize_im(Images_norm[f,:,:], test_mean, test_std)\n","        Images_upsampled[f,:,:] = np.kron(Images_norm[f,:,:], np.ones((upsampling_factor,upsampling_factor)))\n","        Widefield += Images[b*batch_size+f,:,:]\n","\n","      # Reshaping\n","      Images_upsampled = np.expand_dims(Images_upsampled,axis=3)\n","\n","      # Run prediction and local amxima finding\n","      predicted_density = model.predict_on_batch(Images_upsampled)\n","      predicted_density[predicted_density < 0] = 0\n","      Prediction += predicted_density.sum(axis = 3).sum(axis = 0)\n","      \n","      Stack.extend(array for array in predicted_density)\n","      bind, xind, yind, confidence = max_layer(predicted_density)\n","      \n","      # normalizing the confidence by the L2_weighting_factor\n","      confidence /= L2_weighting_factor \n","\n","      # turn indices to nms and append to the results\n","      xind, yind = xind*pixel_size_hr, yind*pixel_size_hr\n","      frmind = (bind.numpy() + b*batch_size + 1).tolist()\n","      xind = xind.numpy().tolist()\n","      yind = yind.numpy().tolist()\n","      confidence = confidence.numpy().tolist()\n","      frame_number_list += frmind\n","      x_nm_list += xind\n","      y_nm_list += yind\n","      confidence_au_list += confidence\n","\n","    # Open and create the csv file that will contain all the localizations\n","    if use_local_avg:\n","      ext = '_avg'\n","    else:\n","      ext = '_max'\n","    with open(os.path.join(savePath, 'Localizations_' + os.path.splitext(filename)[0] + ext + '.csv'), \"w\", newline='') as file:\n","      writer = csv.writer(file)\n","      writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n","      locs = list(zip(frame_number_list, x_nm_list, y_nm_list, confidence_au_list))\n","      writer.writerows(locs)\n","\n","    # Save the prediction and widefield image\n","    Widefield = np.kron(Widefield, np.ones((upsampling_factor,upsampling_factor)))\n","    Widefield = np.float32(Widefield)\n","\n","    # io.imsave(os.path.join(savePath, 'Predicted_'+os.path.splitext(filename)[0]+'.tif'), Prediction)\n","    # io.imsave(os.path.join(savePath, 'Widefield_'+os.path.splitext(filename)[0]+'.tif'), Widefield)\n","\n","    Stack = np.stack(Stack)\n","    #print(Stack.shape)\n","    saveAsTIF(savePath, 'Predicted_'+os.path.splitext(filename)[0], Prediction, pixel_size_hr)\n","    saveAsTIF(savePath, 'Widefield_'+os.path.splitext(filename)[0], Widefield, pixel_size_hr)\n","    saveAsTIF(savePath, 'Stak_'+os.path.splitext(filename)[0], Stack[:,:,:,0], pixel_size_hr)\n","\n","\n","    return\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","  NORMAL = '\\033[0m'  # white (normal)\n","\n","\n","\n","def list_files(directory, extension):\n","  return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n","\n","\n","# @njit(parallel=True)\n","def subPixelMaxLocalization(array, method = 'CoM', patch_size = 3):\n","  xMaxInd, yMaxInd = np.unravel_index(array.argmax(), array.shape, order='C')\n","  centralPatch = XC[(xMaxInd-patch_size):(xMaxInd+patch_size+1),(yMaxInd-patch_size):(yMaxInd+patch_size+1)]\n","\n","  if (method == 'MAX'):\n","    x0 = xMaxInd\n","    y0 = yMaxInd\n","\n","  elif (method == 'CoM'):\n","    x0 = 0\n","    y0 = 0\n","    S = 0\n","    for xy in range(patch_size*patch_size):\n","      y = math.floor(xy/patch_size)\n","      x = xy - y*patch_size\n","      x0 += x*array[x,y]\n","      y0 += y*array[x,y]\n","      S = array[x,y]\n","    \n","    x0 = x0/S - patch_size/2 + xMaxInd\n","    y0 = y0/S - patch_size/2 + yMaxInd\n","  \n","  elif (method == 'Radiality'):\n","    # Not implemented yet\n","    x0 = xMaxInd\n","    y0 = yMaxInd\n","  \n","  return (x0, y0)\n","\n","\n","@njit(parallel=True)\n","def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n","  n_locs = xc_array.shape[0]\n","  xc_array_Corr = np.empty(n_locs)\n","  yc_array_Corr = np.empty(n_locs)\n","  \n","  for loc in prange(n_locs):\n","    xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc]]\n","    yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc]]\n","\n","  return (xc_array_Corr, yc_array_Corr)\n","\n","\n","print('--------------------------------')\n","print('DeepCEL0 installation complete.')\n","\n","# Check if this is the latest version of the notebook\n","#Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n","\n","#if Notebook_version == list(Latest_notebook_version.columns):\n","#  print(\"This notebook is up-to-date.\")\n","\n","#if not Notebook_version == list(Latest_notebook_version.columns):\n","#  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","\n","# Exporting requirements.txt for local run\n","!pip freeze > requirements.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vu8f5NGJkJos"},"source":["\n","# **3. Generate patches for training**\n","---\n","\n","For DeepCEL0 the training data can be obtained in two ways:\n","* Simulated using ThunderSTORM or other simulation tool and loaded here (**using Section 3.1.a**)\n","* Directly simulated in this notebook (**using Section 3.1.b**)\n"]},{"cell_type":"markdown","metadata":{"id":"WSV8xnlynp0l"},"source":["## **3.1.a Load training data**\n","---\n","\n","Here you can load your simulated data along with its corresponding localization file.\n","*   The `pixel_size` is defined in nanometer (nm). "]},{"cell_type":"code","metadata":{"id":"CT6SNcfNg6j0"},"source":["#@markdown ##Load raw data\n","\"\"\"\n","load_raw_data = True\n","\n","# Get user input\n","ImageData_path = \"\" #@param {type:\"string\"}\n","LocalizationData_path = \"\" #@param {type: \"string\"}\n","#@markdown Get pixel size from file?\n","get_pixel_size_from_file = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, use this value:\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","if get_pixel_size_from_file:\n","  pixel_size,_,_ = getPixelSizeTIFFmetadata(ImageData_path, True)\n","\n","# load the tiff data\n","Images = io.imread(ImageData_path)\n","# get dataset dimensions\n","if len(Images.shape) == 3:\n","  (number_of_frames, M, N) = Images.shape\n","elif len(Images.shape) == 2:\n","  (M, N) = Images.shape\n","  number_of_frames = 1\n","print('Loaded images: '+str(M)+'x'+str(N)+' with '+str(number_of_frames)+' frames')\n","\n","# Interactive display of the stack\n","def scroll_in_time(frame):\n","    f=plt.figure(figsize=(6,6))\n","    plt.imshow(Images[frame-1], interpolation='nearest', cmap = 'gray')\n","    plt.title('Training source at frame = ' + str(frame))\n","    plt.axis('off');\n","\n","if number_of_frames > 1:\n","  interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n","else:\n","  f=plt.figure(figsize=(6,6))\n","  plt.imshow(Images, interpolation='nearest', cmap = 'gray')\n","  plt.title('Training source')\n","  plt.axis('off');\n","\n","# Load the localization file and display the first\n","LocData = pd.read_csv(LocalizationData_path, index_col=0)\n","LocData.tail()\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9xE5GeYiks9"},"source":["## **3.1.b Simulate training data**\n","---\n","This simulation tool allows you to generate SMLM data of randomly distrubuted emitters in a field-of-view. \n","The assumptions are as follows:\n","\n","*   Gaussian Point Spread Function (PSF) with standard deviation defined by `Sigma`. The nominal value of `sigma` can be evaluated using `sigma = 0.21 x Lambda / NA`. \n","*   Each emitter will emit `n_photons` per frame, and generate their equivalent Poisson noise.\n","*   The camera will contribute Gaussian noise to the signal with a standard deviation defined by `ReadOutNoise_ADC` in ADC\n","*   The `emitter_density` is defined as the number of emitters / um^2 on any given frame. Variability in the emitter density can be applied by adjusting `emitter_density_std`. The latter parameter represents the standard deviation of the normal distribution that the density is drawn from for each individual frame. `emitter_density` **is defined in number of emitters / um^2**.\n","*   The `n_photons` and `sigma` can additionally include some Gaussian variability by setting `n_photons_std` and `sigma_std`.\n","\n","Important note:\n","- All dimensions are in nanometer (e.g. `FOV_size` = 6400 represents a field of view of 6.4 um x 6.4 um).\n","\n"]},{"cell_type":"code","metadata":{"id":"sQyLXpEhitsg","cellView":"form"},"source":["load_raw_data = False\n","\n","# ---------------------------- User input ----------------------------\n","#@markdown Run the simulation\n","#@markdown --- \n","#@markdown Camera settings: \n","FOV_size =  6400#@param {type:\"number\"}\n","pixel_size =  100#@param {type:\"number\"}\n","ADC_per_photon_conversion = 1 #@param {type:\"number\"}\n","ReadOutNoise_ADC =  4.5#@param {type:\"number\"}\n","ADC_offset =  50#@param {type:\"number\"}\n","\n","#@markdown Acquisition settings: \n","emitter_density =  6#@param {type:\"number\"}\n","emitter_density_std =  0#@param {type:\"number\"}\n","\n","number_of_frames =  20#@param {type:\"integer\"}\n","\n","sigma = 110 #@param {type:\"number\"}\n","sigma_std = 5 #@param {type:\"number\"}\n","# NA =  1.1 #@param {type:\"number\"}\n","# wavelength =  800#@param {type:\"number\"}\n","# wavelength_std =  150#@param {type:\"number\"}\n","n_photons =  2250#@param {type:\"number\"}\n","n_photons_std =  250#@param {type:\"number\"}\n","\n","#@markdown Alignment: \n","display_upsampling=4 #@param {type:\"number\"}\n","\n","\n","# ---------------------------- Variable initialisation ----------------------------\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","print('-----------------------------------------------------------')\n","n_molecules = emitter_density*FOV_size*FOV_size/10**6\n","n_molecules_std = emitter_density_std*FOV_size*FOV_size/10**6\n","print('Number of molecules / FOV: '+str(round(n_molecules,2))+' +/- '+str((round(n_molecules_std,2))))\n","\n","# sigma = 0.21*wavelength/NA\n","# sigma_std = 0.21*wavelength_std/NA\n","# print('Gaussian PSF sigma: '+str(round(sigma,2))+' +/- '+str(round(sigma_std,2))+' nm')\n","\n","M = N = round(FOV_size/pixel_size)\n","FOV_size = M*pixel_size\n","print('Final image size: '+str(M)+'x'+str(M)+' ('+str(round(FOV_size/1000, 3))+'um x'+str(round(FOV_size/1000,3))+' um)')\n","\n","np.random.seed(1)\n","#display_upsampling = 4 # used to display the loc map here\n","NoiseFreeImages = np.zeros((number_of_frames, M, M))\n","locImage = np.zeros((number_of_frames, display_upsampling*M, display_upsampling*N))\n","\n","frames = []\n","all_xloc = []\n","all_yloc = []\n","all_photons = []\n","all_sigmas = []\n","\n","# ---------------------------- Main simulation loop ----------------------------\n","print('-----------------------------------------------------------')\n","for f in tqdm(range(number_of_frames)):\n","  \n","  # Define the coordinates of emitters by randomly distributing them across the FOV\n","  n_mol = int(max(round(np.random.normal(n_molecules, n_molecules_std, size=1)[0]), 0))\n","  x_c = np.random.uniform(low=0.0, high=FOV_size, size=n_mol)\n","  y_c = np.random.uniform(low=0.0, high=FOV_size, size=n_mol)\n","  photon_array = np.random.normal(n_photons, n_photons_std, size=n_mol)\n","  sigma_array = np.random.normal(sigma, sigma_std, size=n_mol)\n","  # x_c = np.linspace(0,3000,5)\n","  # y_c = np.linspace(0,3000,5)\n","\n","  all_xloc += x_c.tolist()\n","  all_yloc += y_c.tolist()\n","  frames += ((f+1)*np.ones(x_c.shape[0])).tolist()\n","  all_photons += photon_array.tolist()\n","  all_sigmas += sigma_array.tolist()\n","\n","  locImage[f] = FromLoc2Image_SimpleHistogram(x_c, y_c, image_size = (N*display_upsampling, M*display_upsampling), pixel_size = pixel_size/display_upsampling)\n","\n","  # # Get the approximated locations according to the grid pixel size\n","  # Chr_emitters = [int(max(min(round(display_upsampling*x_c[i]/pixel_size),N*display_upsampling-1),0)) for i in range(len(x_c))]\n","  # Rhr_emitters = [int(max(min(round(display_upsampling*y_c[i]/pixel_size),M*display_upsampling-1),0)) for i in range(len(y_c))]\n","\n","  # # Build Localization image\n","  # for (r,c) in zip(Rhr_emitters, Chr_emitters):\n","  #   locImage[f][r][c] += 1\n","\n","  NoiseFreeImages[f] = FromLoc2Image_Erf(x_c, y_c, photon_array, sigma_array, image_size = (M,M), pixel_size = pixel_size)\n","\n","\n","# ---------------------------- Create DataFrame fof localization file ----------------------------\n","# Table with localization info as dataframe output\n","LocData = pd.DataFrame()\n","LocData[\"frame\"] = frames\n","LocData[\"x [nm]\"] = all_xloc\n","LocData[\"y [nm]\"] = all_yloc\n","LocData[\"Photon #\"] = all_photons\n","LocData[\"Sigma [nm]\"] = all_sigmas\n","LocData.index += 1  # set indices to start at 1 and not 0 (same as ThunderSTORM)\n","\n","\n","# ---------------------------- Estimation of SNR ----------------------------\n","n_frames_for_SNR = 100\n","M_SNR = 10\n","x_c = np.random.uniform(low=0.0, high=pixel_size*M_SNR, size=n_frames_for_SNR)\n","y_c = np.random.uniform(low=0.0, high=pixel_size*M_SNR, size=n_frames_for_SNR)\n","photon_array = np.random.normal(n_photons, n_photons_std, size=n_frames_for_SNR)\n","sigma_array = np.random.normal(sigma, sigma_std, size=n_frames_for_SNR)\n","\n","SNR = np.zeros(n_frames_for_SNR)\n","for i in range(n_frames_for_SNR):\n","  SingleEmitterImage = FromLoc2Image_Erf(np.array([x_c[i]]), np.array([x_c[i]]), np.array([photon_array[i]]), np.array([sigma_array[i]]), (M_SNR, M_SNR), pixel_size)\n","  Signal_photon = np.max(SingleEmitterImage)\n","  Noise_photon = math.sqrt((ReadOutNoise_ADC/ADC_per_photon_conversion)**2 + Signal_photon)\n","  SNR[i] = Signal_photon/Noise_photon\n","\n","print('SNR: '+str(round(np.mean(SNR),2))+' +/- '+str(round(np.std(SNR),2)))\n","# ---------------------------- ----------------------------\n","\n","\n","# Table with info\n","simParameters = pd.DataFrame()\n","simParameters[\"FOV size (nm)\"] = [FOV_size]\n","simParameters[\"Pixel size (nm)\"] = [pixel_size]\n","simParameters[\"ADC/photon\"] = [ADC_per_photon_conversion]\n","simParameters[\"Read-out noise (ADC)\"] = [ReadOutNoise_ADC]\n","simParameters[\"Constant offset (ADC)\"] = [ADC_offset]\n","\n","simParameters[\"Emitter density (emitters/um^2)\"] = [emitter_density]\n","simParameters[\"STD of emitter density (emitters/um^2)\"] = [emitter_density_std]\n","simParameters[\"Number of frames\"] = [number_of_frames]\n","# simParameters[\"NA\"] = [NA]\n","# simParameters[\"Wavelength (nm)\"] = [wavelength]\n","# simParameters[\"STD of wavelength (nm)\"] = [wavelength_std]\n","simParameters[\"Sigma (nm))\"] = [sigma]\n","simParameters[\"STD of Sigma (nm))\"] = [sigma_std]\n","simParameters[\"Number of photons\"] = [n_photons]\n","simParameters[\"STD of number of photons\"] = [n_photons_std]\n","simParameters[\"SNR\"] = [np.mean(SNR)]\n","simParameters[\"STD of SNR\"] = [np.std(SNR)]\n","\n","\n","# ---------------------------- Finish simulation ----------------------------\n","# Calculating the noisy image\n","Images = ADC_per_photon_conversion * np.random.poisson(NoiseFreeImages) + ReadOutNoise_ADC * np.random.normal(size = (number_of_frames, M, N)) + ADC_offset\n","Images[Images <= 0] = 0\n","\n","# Convert to 16-bit or 32-bits integers\n","if Images.max() < (2**16-1):\n","  Images = Images.astype(np.uint16)\n","else:\n","  Images = Images.astype(np.uint32)\n","\n","\n","# ---------------------------- Display ----------------------------\n","# Displaying the time elapsed for simulation\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds,1),\"sec(s)\")\n","\n","\n","# Interactively display the results using Widgets\n","def scroll_in_time(frame):\n","  f = plt.figure(figsize=(18,6))\n","  plt.subplot(1,3,1)\n","  plt.imshow(locImage[frame-1], interpolation='bilinear', vmin = 0, vmax=0.1)\n","  plt.title('Localization image')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,2)\n","  plt.imshow(NoiseFreeImages[frame-1], interpolation='nearest', cmap='gray')\n","  plt.title('Noise-free simulation')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,3)\n","  plt.imshow(Images[frame-1], interpolation='nearest', cmap='gray')\n","  plt.title('Noisy simulation')\n","  plt.axis('off');\n","\n","interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n","\n","# Display the head of the dataframe with localizations\n","LocData.tail()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pz7RfSuoeJeq","cellView":"form"},"source":["\n","# @markdown ---\n","# @markdown #Play this cell to save the simulated stack\n","# @markdown ####Please select a path to the folder where to save the simulated data. It is not necessary to save the data to run the training, but keeping the simulated for your own record can be useful to check its validity.\n","Save_path = \"dataset\" #@param {type:\"string\"}\n","\n","if not os.path.exists(Save_path):\n","  os.makedirs(Save_path)\n","  print('Folder created.')\n","else:\n","  print('Training data already exists in folder: Data overwritten.')\n","\n","saveAsTIF(Save_path, 'SimulatedDataset', Images, pixel_size)\n","# io.imsave(os.path.join(Save_path, 'SimulatedDataset.tif'),Images)\n","LocData.to_csv(os.path.join(Save_path, 'SimulatedDataset.csv'))\n","simParameters.to_csv(os.path.join(Save_path, 'SimulatedParameters.csv'))\n","print('Training dataset saved.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_8e3kE-JhVY"},"source":["## **3.2. Generate training patches**\n","---\n","\n","Training patches need to be created from the training data generated above. \n","*   The `patch_size` needs to give sufficient contextual information and for most cases a `patch_size` of 26 (corresponding to patches of 26x26 pixels) works fine. **DEFAULT: 26**\n","*   The `upsampling_factor` defines the effective magnification of the final super-resolved image compared to the input image (this is called magnification in ThunderSTORM). This is used to generate the super-resolved patches as target dataset. Using an `upsampling_factor` of 16 will require the use of more memory and it may be necessary to decreae the `patch_size` to 16 for example. **DEFAULT: 8**\n","*   The `num_patches_per_frame` defines the number of patches extracted from each frame generated in section 3.1. **DEFAULT: 500**\n","*   The `min_number_of_emitters_per_patch` defines the minimum number of emitters that need to be present in the patch to be a valid patch. An empty patch does not contain useful information for the network to learn from. **DEFAULT: 7**\n","*   The `max_num_patches` defines the maximum number of patches to generate. Fewer may be generated depending on how many pacthes are rejected and how many frames are available. **DEFAULT: 10000**\n","*   The `gaussian_sigma` defines the Gaussian standard deviation (in magnified pixels) applied to generate the super-resolved target image. **DEFAULT: 1**\n","*   The `L2_weighting_factor` is a normalization factor used in the loss function. It helps balancing the loss from the L2 norm. When using higher densities, this factor should be decreased and vice-versa. This factor can be autimatically calculated using an empiraical formula. **DEFAULT: 100**\n","\n"]},{"cell_type":"code","metadata":{"id":"AsNx5KzcFNvC","cellView":"form"},"source":["#@markdown ## **Provide patch parameters**\n","\n","\n","# -------------------- User input --------------------\n","patch_size = 26 #@param {type:\"integer\"}\n","upsampling_factor = 4 #@param [\"4\", \"8\", \"16\"] {type:\"raw\"}\n","num_patches_per_frame =  500#@param {type:\"integer\"}\n","min_number_of_emitters_per_patch = 7#@param {type:\"integer\"}\n","max_num_patches =  10000#@param {type:\"integer\"}\n","gaussian_sigma = 1#@param {type:\"integer\"}\n","\n","#@markdown Estimate the optimal normalization factor automatically?\n","Automatic_normalization = True #@param {type:\"boolean\"}\n","#@markdown Otherwise, it will use the following value:\n","L2_weighting_factor = 100 #@param {type:\"number\"}\n","\n","\n","# -------------------- Prepare variables --------------------\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","# Initialize some parameters\n","pixel_size_hr = pixel_size/upsampling_factor # in nm\n","n_patches = min(number_of_frames*num_patches_per_frame, max_num_patches)\n","patch_size = patch_size*upsampling_factor\n","\n","# Dimensions of the high-res grid\n","Mhr = upsampling_factor*M # in pixels\n","Nhr = upsampling_factor*N # in pixels\n","\n","# Initialize the training patches and labels\n","patches = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","spikes = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","heatmaps = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n","\n","# Run over all frames and construct the training examples\n","k = 1 # current patch count\n","skip_counter = 0 # number of dataset skipped due to low density\n","id_start = 0 # id position in LocData for current frame\n","print('Generating '+str(n_patches)+' patches of '+str(patch_size)+'x'+str(patch_size))\n","\n","n_locs = len(LocData.index)\n","print('Total number of localizations: '+str(n_locs))\n","density = n_locs/(M*N*number_of_frames*(0.001*pixel_size)**2)\n","print('Density: '+str(round(density,2))+' locs/um^2')\n","n_locs_per_patch = patch_size**2*density\n","\n","if Automatic_normalization:\n","  # This empirical formulae attempts to balance the loss L2 function between the background and the bright spikes\n","  # A value of 100 was originally chosen to balance L2 for a patch size of 2.6x2.6^2 0.1um pixel size and density of 3 (hence the 20.28), at upsampling_factor = 8\n","  L2_weighting_factor = 100/math.sqrt(min(n_locs_per_patch, min_number_of_emitters_per_patch)*8**2/(upsampling_factor**2*20.28))\n","  print('Normalization factor: '+str(round(L2_weighting_factor,2)))\n","\n","# -------------------- Patch generation loop --------------------\n","\n","print('-----------------------------------------------------------')\n","for (f, thisFrame) in enumerate(tqdm(Images)):\n","\n","  # Upsample the frame\n","  upsampledFrame = np.kron(thisFrame, np.ones((upsampling_factor,upsampling_factor)))\n","  # Read all the provided high-resolution locations for current frame\n","  DataFrame = LocData[LocData['frame'] == f+1].copy()\n","\n","  # Get the approximated locations according to the high-res grid pixel size\n","  Chr_emitters = [int(max(min(round(DataFrame['x [nm]'][i]/pixel_size_hr),Nhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n","  Rhr_emitters = [int(max(min(round(DataFrame['y [nm]'][i]/pixel_size_hr),Mhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n","  id_start += len(DataFrame.index)\n","\n","  # Build Localization image\n","  LocImage = np.zeros((Mhr,Nhr))\n","  LocImage[(Rhr_emitters, Chr_emitters)] = 1\n","\n","  # Here, there's a choice between the original Gaussian (classification approach) and using the erf function\n","  HeatMapImage = L2_weighting_factor*gaussian_filter(LocImage, gaussian_sigma)  \n","  # HeatMapImage = L2_weighting_factor*FromLoc2Image_MultiThreaded(np.array(list(DataFrame['x [nm]'])), np.array(list(DataFrame['y [nm]'])), \n","                                                            #  np.ones(len(DataFrame.index)), pixel_size_hr*gaussian_sigma*np.ones(len(DataFrame.index)), \n","                                                            #  Mhr, pixel_size_hr)\n","  \n","\n","  # Generate random position for the top left corner of the patch\n","  xc = np.random.randint(0, Mhr-patch_size, size=num_patches_per_frame)\n","  yc = np.random.randint(0, Nhr-patch_size, size=num_patches_per_frame)\n","\n","  for c in range(len(xc)):\n","    if LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size].sum() < min_number_of_emitters_per_patch:\n","      skip_counter += 1\n","      continue\n","    \n","    else:\n","        # Limit maximal number of training examples to 15k\n","      if k > max_num_patches:\n","        break\n","      else:\n","        # Assign the patches to the right part of the images\n","        patches[k-1] = upsampledFrame[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        spikes[k-1] = LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        heatmaps[k-1] = HeatMapImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n","        k += 1 # increment current patch count\n","\n","# Remove the empty data\n","patches = patches[:k-1]\n","spikes = spikes[:k-1]\n","heatmaps = heatmaps[:k-1]\n","n_patches = k-1\n","\n","# -------------------- Failsafe --------------------\n","# Check if the size of the training set is smaller than 5k to notify user to simulate more images using ThunderSTORM\n","if ((k-1) < 5000):\n","  # W  = '\\033[0m'  # white (normal)\n","  # R  = '\\033[31m' # red\n","  print(bcolors.WARNING+'!! WARNING: Training set size is below 5K - Consider simulating more images in ThunderSTORM. !!'+bcolors.NORMAL)\n","\n","\n","\n","# -------------------- Displays --------------------\n","print('Number of patches skipped due to low density: '+str(skip_counter))\n","# dataSize = int((getsizeof(patches)+getsizeof(heatmaps)+getsizeof(spikes))/(1024*1024)) #rounded in MB\n","# print('Size of patches: '+str(dataSize)+' MB')\n","print(str(n_patches)+' patches were generated.')\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","# Display patches interactively with a slider\n","def scroll_patches(patch):\n","  f = plt.figure(figsize=(16,6))\n","  plt.subplot(1,3,1)\n","  plt.imshow(patches[patch-1], interpolation='nearest', cmap='gray')\n","  plt.title('Raw data (frame #'+str(patch)+')')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,2)\n","  plt.imshow(heatmaps[patch-1], interpolation='nearest')\n","  plt.title('Heat map')\n","  plt.axis('off');\n","\n","  plt.subplot(1,3,3)\n","  plt.imshow(spikes[patch-1], interpolation='nearest')\n","  plt.title('Localization map')\n","  plt.axis('off');\n","  \n","  plt.savefig('/content/TrainingDataExample_DeepSTORM2D.png',bbox_inches='tight',pad_inches=0)\n","\n","\n","interact(scroll_patches, patch=widgets.IntSlider(min=1, max=patches.shape[0], step=1, value=0, continuous_update=False));\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSjXFMevK7Iz"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"markdown","metadata":{"id":"hVeyKU0MdAPx"},"source":["## **4.1. Select your paths and parameters**\n","\n","---\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n","\n","\n","<font size = 5>**Training parameters**\n","\n","<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for ~100 epochs. Evaluate the performance after training (see 5). **Default value: 80**\n","\n","<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 16**\n","\n","<font size = 4>**`number_of_steps`:** Define the number of training steps by epoch. **If this value is set to 0**, by default this parameter is calculated so that each patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n","\n","<font size = 4>**`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Default value: 30** \n","\n","<font size = 4>**`initial_learning_rate`:** This parameter represents the initial value to be used as learning rate in the optimizer. **Default value: 0.001**"]},{"cell_type":"code","metadata":{"id":"oa5cDZ7f_PF6","cellView":"form"},"source":["#@markdown ###Path to training images and parameters\n","\n","model_path = \"weights/myweights\" #@param {type: \"string\"} \n","model_name = \"mymodel\" #@param {type: \"string\"} \n","number_of_epochs =  100#@param {type:\"integer\"}\n","batch_size =  16#@param {type:\"integer\"}\n","\n","number_of_steps =  0#@param {type:\"integer\"}\n","percentage_validation = 30 #@param {type:\"number\"}\n","initial_learning_rate = 0.001 #@param {type:\"number\"}\n","L0_weight =  100#@param {type:\"number\"}\n","\n","percentage_validation /= 100\n","if number_of_steps == 0: \n","  number_of_steps = int((1-percentage_validation)*n_patches/batch_size)\n","  print('Number of steps: '+str(number_of_steps))\n","\n","# Pretrained model path initialised here so next cell does not need to be run\n","h5_file_path = ''\n","Use_pretrained_model = False\n","\n","if not ('patches' in locals()):\n","  # W  = '\\033[0m'  # white (normal)\n","  # R  = '\\033[31m' # red\n","  print(WARNING+'!! WARNING: No patches were found in memory currently. !!')\n","\n","Save_path = os.path.join(model_path, model_name)\n","if os.path.exists(Save_path):\n","  print(bcolors.WARNING+'The model folder already exists and will be overwritten.'+bcolors.NORMAL)\n","\n","print('-----------------------------')\n","print('Training parameters set.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIyEvQBWLp9n"},"source":["\n","## **4.2. Using weights from a pre-trained model as initial weights**\n","---\n","<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a DeepCEL0 2D model**. \n","\n","<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n","\n","<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used. "]},{"cell_type":"code","metadata":{"cellView":"form","id":"oHL5g0w8LqR0"},"source":["# @markdown ##Loading weights from a pre-trained network\n","\n","Use_pretrained_model = True #@param {type:\"boolean\"}\n","pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\"]\n","Weights_choice = \"best\" #@param [\"last\", \"best\"]\n","\n","#@markdown ###If you chose \"Model_from_file\", please provide the path to the model folder:\n","pretrained_model_path = \"weights/myweights\" #@param {type:\"string\"}\n","\n","# --------------------- Check if we load a previously trained model ------------------------\n","if Use_pretrained_model:\n","\n","# --------------------- Load the model from the choosen path ------------------------\n","  if pretrained_model_choice == \"Model_from_file\":\n","    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n","\n","# --------------------- Download the a model provided in the XXX ------------------------\n","\n","  if pretrained_model_choice == \"Model_name\":\n","    pretrained_model_name = \"Model_name\"\n","    pretrained_model_path = \"/content/\"+pretrained_model_name\n","    print(\"Downloading the 2D_Demo_Model_from_Stardist_2D_paper\")\n","    if os.path.exists(pretrained_model_path):\n","      shutil.rmtree(pretrained_model_path)\n","    os.makedirs(pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)\n","    wget.download(\"\", pretrained_model_path)    \n","    wget.download(\"\", pretrained_model_path)\n","    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n","\n","# --------------------- Add additional pre-trained models here ------------------------\n","\n","\n","\n","# --------------------- Check the model exist ------------------------\n","# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled, \n","  if not os.path.exists(h5_file_path):\n","    print(bcolors.WARNING+'WARNING: weights_'+Weights_choice+'.hdf5 pretrained model does not exist'+bcolors.NORMAL)\n","    Use_pretrained_model = False\n","\n","  \n","# If the model path contains a pretrain model, we load the training rate, \n","  if os.path.exists(h5_file_path):\n","#Here we check if the learning rate can be loaded from the quality control folder\n","    if os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n","      with open(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n","        csvRead = pd.read_csv(csvfile, sep=',')\n","        #print(csvRead)\n","        if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n","          print(\"pretrained network learning rate found\")\n","          #find the last learning rate\n","          lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n","          #Find the learning rate corresponding to the lowest validation loss\n","          min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n","          #print(min_val_loss)\n","          bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n","          if Weights_choice == \"last\":\n","            print('Last learning rate: '+str(lastLearningRate))\n","          if Weights_choice == \"best\":\n","            print('Learning rate of best validation loss: '+str(bestLearningRate))\n","        if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n","          bestLearningRate = initial_learning_rate\n","          lastLearningRate = initial_learning_rate\n","          print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead.'+bcolors.NORMAL)\n","\n","#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n","    if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n","      print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(initial_learning_rate)+' will be used instead'+bcolors.NORMAL)\n","      bestLearningRate = initial_learning_rate\n","      lastLearningRate = initial_learning_rate\n","\n","\n","# Display info about the pretrained model to be loaded (or not)\n","if Use_pretrained_model:\n","  print('Weights found in:')\n","  print(h5_file_path)\n","  print('will be loaded prior to training.')\n","\n","else:\n","  print('No pretrained network will be used.')\n","  h5_file_path = ''\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OADNcie-LHxA"},"source":["## **4.4. Start Training**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches."]},{"cell_type":"code","metadata":{"id":"qDgMu_mAK8US"},"source":["##Start training\n","\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","# --------------------- Using pretrained model ------------------------\n","#Here we ensure that the learning rate set correctly when using pre-trained models\n","if Use_pretrained_model:\n","  if Weights_choice == \"last\":\n","    initial_learning_rate = lastLearningRate\n","\n","  if Weights_choice == \"best\":            \n","    initial_learning_rate = bestLearningRate\n","# --------------------- ---------------------- ------------------------\n","\n","\n","#here we check that no model with the same name already exist, if so delete\n","if os.path.exists(Save_path):\n","  shutil.rmtree(Save_path)\n","\n","# Create the model folder!\n","os.makedirs(Save_path)\n","\n","# Let's go !\n","train_model(patches, heatmaps, Save_path, \n","            steps_per_epoch=number_of_steps, epochs=number_of_epochs, batch_size=batch_size,\n","            upsampling_factor = upsampling_factor,\n","            validation_split = percentage_validation,\n","            initial_learning_rate = initial_learning_rate, \n","            pretrained_model_path = h5_file_path,\n","            L2_weighting_factor = L2_weighting_factor,\n","            L0_weight = L0_weight)\n","\n","# # Show info about the GPU memory useage\n","# !nvidia-smi\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","\n","\n","\n","\n","# -------------------------------------------\n","#Create a pdf document with training summary\n","\n","# save FPDF() class into a  \n","# variable pdf \n","\n","class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","pdf = MyFPDF()\n","pdf.add_page()\n","pdf.set_right_margin(-1)\n","pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","Network = 'DeepCEL0'\n","#model_name = 'little_CARE_test'\n","day = datetime.now()\n","datetime_str = str(day)[0:10]\n","\n","Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n","pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","  \n","# add another cell \n","training_time = \"Training time: \"+str(hours)+ \"hour(s) \"+str(minutes)+\"min(s) \"+str(round(seconds))+\"sec(s)\"\n","pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n","pdf.ln(1)\n","\n","Header_2 = 'Information for your materials and method:'\n","pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n","\n","all_packages = ''\n","for requirement in freeze(local_only=True):\n","  all_packages = all_packages+requirement+', '\n","#print(all_packages)\n","\n","#Main Packages\n","main_packages = ''\n","version_numbers = []\n","for name in ['tensorflow','numpy','Keras']:\n","  find_name=all_packages.find(name)\n","  main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n","  #Version numbers only here:\n","  version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n","\n","cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n","cuda_version = cuda_version.stdout.decode('utf-8')\n","cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n","gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n","gpu_name = gpu_name.stdout.decode('utf-8')\n","gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n","#print(cuda_version[cuda_version.find(', V')+3:-1])\n","#print(gpu_name)\n","if load_raw_data == True:\n","  shape = (M,N)\n","else:\n","  shape = (int(FOV_size/pixel_size),int(FOV_size/pixel_size))\n","#dataset_size = len(os.listdir(Training_source))\n","\n","text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), Keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n","\n","if Use_pretrained_model:\n","  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. The models was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), Keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n","\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","pdf.multi_cell(180, 5, txt = text, align='L')\n","pdf.ln(1)\n","pdf.set_font('')\n","pdf.set_font(\"Arial\", size = 11, style='B')\n","pdf.ln(1)\n","pdf.cell(190, 5, txt = 'Training dataset', align='L', ln=1)\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","if load_raw_data==False:\n","  simul_text = 'The training dataset was created in the notebook using the following simulation settings:'\n","  pdf.cell(200, 5, txt=simul_text, align='L')\n","  pdf.ln(1)\n","  html = \"\"\" \n","  <table width=60% style=\"margin-left:0px;\">\n","    <tr>\n","      <th width = 50% align=\"left\">Setting</th>\n","      <th width = 50% align=\"left\">Simulated Value</th>\n","    </tr>\n","    <tr>\n","      <td width = 50%>FOV_size</td>\n","      <td width = 50%>{0}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>pixel_size</td>\n","      <td width = 50%>{1}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>ADC_per_photon_conversion</td>\n","      <td width = 50%>{2}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>ReadOutNoise_ADC</td>\n","      <td width = 50%>{3}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>ADC_offset</td>\n","      <td width = 50%>{4}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>emitter_density</td>\n","      <td width = 50%>{5}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>emitter_density_std</td>\n","      <td width = 50%>{6}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>number_of_frames</td>\n","      <td width = 50%>{7}</td>\n","    </tr> \n","    <tr>\n","      <td width = 50%>sigma</td>\n","      <td width = 50%>{8}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>sigma_std</td>\n","      <td width = 50%>{9}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>n_photons</td>\n","      <td width = 50%>{10}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>n_photons_std</td>\n","      <td width = 50%>{11}</td>\n","    </tr> \n","  </table>\n","  \"\"\".format(FOV_size, pixel_size, ADC_per_photon_conversion, ReadOutNoise_ADC, ADC_offset, emitter_density, emitter_density_std, number_of_frames, sigma, sigma_std, n_photons, n_photons_std)\n","  pdf.write_html(html)\n","else:\n","  simul_text = 'The training dataset was simulated using ThunderSTORM and loaded into the notebook.'\n","  pdf.multi_cell(190, 5, txt=simul_text, align='L')\n","  pdf.set_font(\"Arial\", size = 11, style='B')\n","  #pdf.ln(1)\n","  #pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(29, 5, txt= 'ImageData_path', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = ImageData_path, align = 'L')\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(28, 5, txt= 'LocalizationData_path:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = LocalizationData_path, align = 'L')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(28, 5, txt= 'pixel_size:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = str(pixel_size), align = 'L')\n","#pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n","pdf.set_font('Arial', size = 11, style = 'B')\n","pdf.ln(1)\n","pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n","pdf.set_font('')\n","pdf.set_font_size(10.)\n","# if Use_Default_Advanced_Parameters:\n","#   pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n","pdf.cell(200, 5, txt='The following parameters were used to generate patches:')\n","pdf.ln(1)\n","html = \"\"\"\n","<table width=70% style=\"margin-left:0px;\">\n","  <tr>\n","    <th width = 50% align=\"left\">Patch Parameter</th>\n","    <th width = 50% align=\"left\">Value</th>\n","  </tr>\n","  <tr>\n","    <td width = 50%>patch_size</td>\n","    <td width = 50%>{0}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>upsampling_factor</td>\n","    <td width = 50%>{1}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>num_patches_per_frame</td>\n","    <td width = 50%>{2}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>min_number_of_emitters_per_patch</td>\n","    <td width = 50%>{3}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>max_num_patches</td>\n","    <td width = 50%>{4}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>gaussian_sigma</td>\n","    <td width = 50%>{5}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>Automatic_normalization</td>\n","    <td width = 50%>{6}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>L2_weighting_factor</td>\n","    <td width = 50%>{7}</td>\n","  </tr>\n","\"\"\".format(str(patch_size)+'x'+str(patch_size), upsampling_factor, num_patches_per_frame, min_number_of_emitters_per_patch, max_num_patches, gaussian_sigma, Automatic_normalization, L2_weighting_factor)\n","pdf.write_html(html)\n","pdf.ln(3)\n","pdf.set_font('Arial', size=10)\n","pdf.cell(200, 5, txt='The following parameters were used for training:')\n","pdf.ln(1)\n","html = \"\"\" \n","<table width=70% style=\"margin-left:0px;\">\n","  <tr>\n","    <th width = 50% align=\"left\">Training Parameter</th>\n","    <th width = 50% align=\"left\">Value</th>\n","  </tr>\n","  <tr>\n","    <td width = 50%>number_of_epochs</td>\n","    <td width = 50%>{0}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>batch_size</td>\n","    <td width = 50%>{1}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>number_of_steps</td>\n","    <td width = 50%>{2}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>percentage_validation</td>\n","    <td width = 50%>{3}</td>\n","  </tr>\n","  <tr>\n","    <td width = 50%>initial_learning_rate</td>\n","    <td width = 50%>{4}</td>\n","  </tr>\n","</table>\n","\"\"\".format(number_of_epochs,batch_size,number_of_steps,percentage_validation,initial_learning_rate)\n","pdf.write_html(html)\n","\n","pdf.ln(1)\n","# pdf.set_font('')\n","pdf.set_font('Arial', size = 10, style = 'B')\n","pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n","pdf.set_font('')\n","pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n","\n","pdf.ln(1)\n","pdf.cell(60, 5, txt = 'Example Training Images', ln=1)\n","pdf.ln(1)\n","exp_size = io.imread('/content/TrainingDataExample_DeepSTORM2D.png').shape\n","pdf.image('/content/TrainingDataExample_DeepSTORM2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","pdf.ln(1)\n","ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" bioRxiv (2020).'\n","pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","ref_2 = '- Deep-STORM: Nehme, Elias, et al. \"Deep-STORM: super-resolution single-molecule microscopy by deep learning.\" Optica 5.4 (2018): 458-464.'\n","pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","ref_3 = '- DeepCEL0: Cascarano, Pasquale, Comes, Maria Colomba et al. \"DeepCEL0 for 2D Single Molecule Localization inFluorescence Microscopy'\n","pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","# if Use_Data_augmentation:\n","#   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n","#   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n","pdf.ln(3)\n","reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n","pdf.set_font('Arial', size = 11, style='B')\n","pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n","print('------------------------------')\n","print('PDF report exported in '+model_path+'/'+model_name+'/')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHVTRjEOLRDH"},"source":["##**4.5. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."]},{"cell_type":"markdown","metadata":{"id":"4N7-ShZpLhwr"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**"]},{"cell_type":"code","metadata":{"id":"JDRsm7uKoBa-","cellView":"form"},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the path to the model folder:\n","#@markdown #####During training, the model files are automatically saved inside a folder named after the parameter `model_name` (see section 4.1). Provide the name of this folder as `QC_model_path` . \n","\n","QC_model_path = \"weights/myweights/Quality Control\" #@param {type:\"string\"}\n","\n","if (Use_the_current_trained_model): \n","  QC_model_path = os.path.join(model_path, model_name)\n","\n","if os.path.exists(QC_model_path):\n","  print(\"The \"+os.path.basename(QC_model_path)+\" model will be evaluated\")\n","else:\n","  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n","  print('Please make sure you provide a valid model path before proceeding further.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gw7KaHZUoHC4"},"source":["## **5.1. Inspection of the loss function**\n","---\n","\n","<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n","\n","<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n","\n","<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n","\n","<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n","\n","<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."]},{"cell_type":"code","metadata":{"id":"qUc-JMOcoGNZ","cellView":"form"},"source":["#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n","import csv\n","from matplotlib import pyplot as plt\n","\n","lossDataFromCSV = []\n","vallossDataFromCSV = []\n","\n","with open(os.path.join(QC_model_path,'Quality Control/training_evaluation.csv'),'r') as csvfile:\n","    csvRead = csv.reader(csvfile, delimiter=',')\n","    next(csvRead)\n","    for row in csvRead:\n","        lossDataFromCSV.append(float(row[0]))\n","        vallossDataFromCSV.append(float(row[1]))\n","\n","epochNumber = range(len(lossDataFromCSV))\n","plt.figure(figsize=(15,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","\n","plt.subplot(2,1,2)\n","plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (log scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","plt.savefig(os.path.join(QC_model_path,'Quality Control/lossCurvePlots.png'), bbox_inches='tight', pad_inches=0)\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32eNQjFioQkY"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","\n","<font size = 4>This section will display SSIM maps and RSE maps as well as calculating total SSIM, NRMSE and PSNR metrics for all the images provided in the \"QC_image_folder\" using teh corresponding localization data contained in \"QC_loc_folder\" !\n","\n","<font size = 4>**1. The SSIM (structural similarity) map** \n","\n","<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n","\n","<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n","\n","<font size=4>**The output below shows the SSIM maps with the mSSIM**\n","\n","<font size = 4>**2. The RSE (Root Squared Error) map** \n","\n","<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n","\n","\n","<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n","\n","<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n","\n","<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yTRou0izLjhd"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"eAf8aBDmWTx7"},"source":["## **6.1 Generate image prediction and localizations from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n","\n","<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 4**\n","\n","<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. The value is expected to reside in the range **[0,1]**. A higher `threshold` will result in less localizations. **DEFAULT: 0.1**\n","\n","<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n","\n","<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n"]},{"cell_type":"code","metadata":{"id":"7qn06T_A0lxf","cellView":"form"},"source":["\n","# ------------------------------- User input -------------------------------\n","#@markdown ### Data parameters\n","Data_folder = \"testdata\" #@param {type:\"string\"}\n","Result_folder = \"output/savedata\" #@param {type:\"string\"}\n","#@markdown Get pixel size from file?\n","get_pixel_size_from_file = False #@param {type:\"boolean\"}\n","#@markdown Otherwise, use this value (in nm):\n","pixel_size = 100 #@param {type:\"number\"}\n","\n","#@markdown ### Model parameters\n","#@markdown Do you want to use the model you just trained?\n","Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","#@markdown Otherwise, please provide path to the model folder below\n","prediction_model_path = \"weights/myweights\" #@param {type:\"string\"}\n","\n","#@markdown ### Prediction parameters\n","batch_size =  4#@param {type:\"integer\"}\n","\n","#@markdown ### Post processing parameters\n","threshold =  0.1#@param {type:\"number\"}\n","neighborhood_size =  3#@param {type:\"integer\"}\n","#@markdown Do you want to locally average the model output with CoG estimator ?\n","use_local_average = True #@param {type:\"boolean\"}\n","\n","\n","if get_pixel_size_from_file:\n","  pixel_size = None\n","\n","if (Use_the_current_trained_model): \n","  prediction_model_path = os.path.join(model_path, model_name)\n","\n","if os.path.exists(prediction_model_path):\n","  print(\"The \"+os.path.basename(prediction_model_path)+\" model will be used.\")\n","else:\n","  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n","  print('Please make sure you provide a valid model path before proceeding further.')\n","\n","# inform user whether local averaging is being used\n","if use_local_average == True: \n","  print('Using local averaging')\n","\n","if not os.path.exists(Result_folder):\n","  print('Result folder was created.')\n","  os.makedirs(Result_folder)\n","\n","\n","# ------------------------------- Run predictions -------------------------------\n","\n","start = time.time()\n","#%% This script tests the trained fully convolutional network based on the \n","# saved training weights, and normalization created using train_model.\n","\n","if os.path.isdir(Data_folder): \n","  for filename in list_files(Data_folder, 'tif'):\n","    # run the testing/reconstruction process\n","    print(\"------------------------------------\")\n","    print(\"Running prediction on: \"+ filename)\n","    batchFramesPredictionLocalization(Data_folder, filename, prediction_model_path, Result_folder, \n","                                     batch_size, \n","                                     threshold, \n","                                     neighborhood_size, \n","                                     use_local_average,\n","                                     pixel_size = pixel_size)\n","\n","elif os.path.isfile(Data_folder):\n","  batchFramesPredictionLocalization(os.path.dirname(Data_folder), os.path.basename(Data_folder), prediction_model_path, Result_folder, \n","                                   batch_size, \n","                                   threshold, \n","                                   neighborhood_size, \n","                                   use_local_average, \n","                                   pixel_size = pixel_size)\n","\n","\n","\n","print('--------------------------------------------------------------------')\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n","\n","\n","# ------------------------------- Interactive display -------------------------------\n","\n","print('--------------------------------------------------------------------')\n","print('---------------------------- Previews ------------------------------')\n","print('--------------------------------------------------------------------')\n","\n","if os.path.isdir(Data_folder): \n","  @interact\n","  def show_QC_results(file = list_files(Data_folder, 'tif')):\n","\n","    plt.figure(figsize=(15,7.5))\n","    # Wide-field\n","    plt.subplot(1,2,1)\n","    plt.axis('off')\n","    img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+file))\n","    plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n","    plt.title('Widefield', fontsize=15)\n","    # Prediction\n","    plt.subplot(1,2,2)\n","    plt.axis('off')\n","    img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+file))\n","    plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n","    plt.title('Predicted',fontsize=15)\n","\n","if os.path.isfile(Data_folder):\n","\n","  plt.figure(figsize=(15,7.5))\n","  # Wide-field\n","  plt.subplot(1,2,1)\n","  plt.axis('off')\n","  img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+os.path.basename(Data_folder)))\n","  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n","  plt.title('Widefield', fontsize=15)\n","  # Prediction\n","  plt.subplot(1,2,2)\n","  plt.axis('off')\n","  img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+os.path.basename(Data_folder)))\n","  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n","  plt.title('Predicted',fontsize=15)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EszIF4Dkz_n"},"source":["## **6.4. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]}]}